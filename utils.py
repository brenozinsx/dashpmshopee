import pandas as pd
import streamlit as st
from datetime import datetime
import json
import os
import re
import unicodedata
from config import DADOS, MENSAGENS
import numpy as np

# Importa√ß√£o condicional do database para evitar erros
try:
    from database import db_manager
    DB_AVAILABLE = True
except ImportError:
    DB_AVAILABLE = False
    st.warning("‚ö†Ô∏è M√≥dulo database n√£o dispon√≠vel. Usando apenas armazenamento local.")

def processar_upload_planilha(uploaded_file):
    """
    Processa upload de planilha Excel e retorna dados formatados
    """
    try:
        if uploaded_file.name.endswith('.xlsx'):
            df = pd.read_excel(uploaded_file)
        elif uploaded_file.name.endswith('.csv'):
            df = pd.read_csv(uploaded_file)
        else:
            st.error("Formato de arquivo n√£o suportado. Use .xlsx ou .csv")
            return None
        
        # Mapeamento esperado das colunas
        colunas_esperadas = {
            'data': ['Data', 'DATA', 'data', 'Date', 'DATE'],
            'backlog': ['Backlog', 'BACKLOG', 'backlog', 'Pacotes Anteriores'],
            'volume_veiculo': ['Volume Ve√≠culo', 'VOLUME_VEICULO', 'Volume do Ve√≠culo'],
            'flutuantes': ['Flutuantes', 'FLUTUANTES', 'flutuantes', 'Sem Bipar'],
            'erros_sorting': ['Erros Sorting', 'ERROS_SORTING', 'Erros 2¬∫ Sorting'],
            'erros_etiquetagem': ['Erros Etiquetagem', 'ERROS_ETIQUETAGEM', 'Erros Etiqueta']
        }
        
        # Mapear colunas
        colunas_mapeadas = {}
        for coluna_esperada, possiveis_nomes in colunas_esperadas.items():
            for nome in possiveis_nomes:
                if nome in df.columns:
                    colunas_mapeadas[coluna_esperada] = nome
                    break
        
        if len(colunas_mapeadas) < 6:
            st.error("Planilha deve conter as colunas: Data, Backlog, Volume Ve√≠culo, Flutuantes, Erros Sorting, Erros Etiquetagem")
            return None
        
        # Processar dados
        dados_processados = []
        for _, row in df.iterrows():
            try:
                data = pd.to_datetime(row[colunas_mapeadas['data']]).strftime('%Y-%m-%d')
                backlog = int(row[colunas_mapeadas['backlog']])
                volume_veiculo = int(row[colunas_mapeadas['volume_veiculo']])
                flutuantes = int(row[colunas_mapeadas['flutuantes']])
                erros_sorting = int(row[colunas_mapeadas['erros_sorting']])
                erros_etiquetagem = int(row[colunas_mapeadas['erros_etiquetagem']])
                
                dados_processados.append({
                    'data': data,
                    'backlog': backlog,
                    'volume_veiculo': volume_veiculo,
                    'volume_diario': backlog + volume_veiculo,
                    'flutuantes': flutuantes,
                    'erros_sorting': erros_sorting,
                    'erros_etiquetagem': erros_etiquetagem
                })
            except Exception as e:
                st.warning(f"Erro ao processar linha: {e}")
                continue
        
        return dados_processados
        
    except Exception as e:
        st.error(f"Erro ao processar arquivo: {e}")
        return None

def exportar_dados_excel(dados, nome_arquivo="relatorio_operacao.xlsx"):
    """
    Exporta dados para arquivo Excel
    """
    try:
        df = pd.DataFrame(dados)
        df['data'] = pd.to_datetime(df['data'])
        df = df.sort_values('data')
        
        # Adicionar colunas calculadas
        df['taxa_flutuantes'] = (df['flutuantes'] / df['volume_diario'] * 100).round(2)
        df['taxa_erros_sorting'] = (df['erros_sorting'] / df['volume_diario'] * 100).round(2)
        df['taxa_erros_etiquetagem'] = (df['erros_etiquetagem'] / df['volume_diario'] * 100).round(2)
        
        # Formatar data para exibi√ß√£o
        df['data_formatada'] = df['data'].dt.strftime('%d/%m/%Y')
        
        # Reorganizar colunas
        colunas_ordenadas = [
            'data_formatada', 'backlog', 'volume_veiculo', 'volume_diario',
            'flutuantes', 'taxa_flutuantes', 'erros_sorting', 'taxa_erros_sorting',
            'erros_etiquetagem', 'taxa_erros_etiquetagem'
        ]
        
        df_export = df[colunas_ordenadas].copy()
        df_export.columns = [
            'Data', 'Backlog', 'Volume Ve√≠culo', 'Volume Total',
            'Flutuantes', 'Taxa Flutuantes (%)', 'Erros Sorting', 'Taxa Erros Sorting (%)',
            'Erros Etiquetagem', 'Taxa Erros Etiquetagem (%)'
        ]
        
        # Salvar Excel
        with pd.ExcelWriter(nome_arquivo, engine='xlsxwriter') as writer:
            df_export.to_excel(writer, sheet_name='Dados Opera√ß√£o', index=False)
            
            # Formata√ß√£o
            workbook = writer.book
            worksheet = writer.sheets['Dados Opera√ß√£o']
            
            # Formatos
            formato_header = workbook.add_format({
                'bold': True,
                'bg_color': '#EE4D2D',
                'font_color': 'white',
                'border': 1
            })
            
            formato_percentual = workbook.add_format({
                'num_format': '0.00%',
                'border': 1
            })
            
            formato_data = workbook.add_format({
                'num_format': 'dd/mm/yyyy',
                'border': 1
            })
            
            formato_numero = workbook.add_format({
                'num_format': '#,##0',
                'border': 1
            })
            
            # Aplicar formatos
            for col_num, value in enumerate(df_export.columns.values):
                worksheet.write(0, col_num, value, formato_header)
            
            # Formatar colunas espec√≠ficas
            worksheet.set_column('A:A', 12, formato_data)  # Data
            worksheet.set_column('B:D', 12, formato_numero)  # Volumes
            worksheet.set_column('E:E', 12, formato_numero)  # Flutuantes
            worksheet.set_column('F:F', 15, formato_percentual)  # Taxa flutuantes
            worksheet.set_column('G:G', 12, formato_numero)  # Erros sorting
            worksheet.set_column('H:H', 18, formato_percentual)  # Taxa sorting
            worksheet.set_column('I:I', 15, formato_numero)  # Erros etiquetagem
            worksheet.set_column('J:J', 20, formato_percentual)  # Taxa etiquetagem
        
        return nome_arquivo
        
    except Exception as e:
        st.error(f"Erro ao exportar dados: {e}")
        return None

def gerar_relatorio_resumo(dados):
    """
    Gera relat√≥rio resumo dos dados
    """
    if not dados:
        return None
    
    df = pd.DataFrame(dados)
    df['data'] = pd.to_datetime(df['data'])
    
    # M√©tricas gerais
    total_dias = len(df)
    total_pacotes = df['volume_diario'].sum()
    media_diaria = df['volume_diario'].mean()
    
    # Taxas m√©dias
    taxa_media_flutuantes = (df['flutuantes'].sum() / total_pacotes * 100)
    taxa_media_sorting = (df['erros_sorting'].sum() / total_pacotes * 100)
    taxa_media_etiquetagem = (df['erros_etiquetagem'].sum() / total_pacotes * 100)
    
    # Melhor e pior dia
    melhor_dia = df.loc[df['flutuantes'].idxmin()]
    pior_dia = df.loc[df['flutuantes'].idxmax()]
    
    relatorio = {
        'periodo_analise': f"{df['data'].min().strftime('%d/%m/%Y')} a {df['data'].max().strftime('%d/%m/%Y')}",
        'total_dias': total_dias,
        'total_pacotes': total_pacotes,
        'media_diaria': media_diaria,
        'taxa_media_flutuantes': taxa_media_flutuantes,
        'taxa_media_sorting': taxa_media_sorting,
        'taxa_media_etiquetagem': taxa_media_etiquetagem,
        'melhor_dia': {
            'data': melhor_dia['data'].strftime('%d/%m/%Y'),
            'flutuantes': melhor_dia['flutuantes'],
            'volume': melhor_dia['volume_diario']
        },
        'pior_dia': {
            'data': pior_dia['data'].strftime('%d/%m/%Y'),
            'flutuantes': pior_dia['flutuantes'],
            'volume': pior_dia['volume_diario']
        }
    }
    
    return relatorio

def backup_dados(dados):
    """
    Cria backup dos dados
    """
    try:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        nome_backup = f"backup_dados_{timestamp}.json"
        
        with open(nome_backup, 'w', encoding='utf-8') as f:
            json.dump(dados, f, ensure_ascii=False, indent=2)
        
        st.success(f"‚úÖ Backup criado: {nome_backup}")
        return nome_backup
    except Exception as e:
        st.error(f"‚ùå **ERRO AO CRIAR BACKUP:**")
        st.error(f"**Tipo:** {type(e).__name__}")
        st.error(f"**Mensagem:** {str(e)}")
        return None

# ============================================================================
# FUN√á√ïES DE INTEGRA√á√ÉO COM BANCO DE DADOS
# ============================================================================

def salvar_dados_operacao(dados: list) -> bool:
    """
    Salva dados de opera√ß√£o no banco de dados e localmente
    """
    try:
        st.info(f"üîÑ Salvando {len(dados)} registros...")
        
        # Salvar localmente (backup)
        if DADOS['backup_automatico']:
            st.info("üì¶ Criando backup...")
            backup_dados(dados)
        
        # Salvar no Supabase se conectado
        if DB_AVAILABLE and db_manager.is_connected():
            st.info("üåê Tentando salvar no Supabase...")
            success = db_manager.save_dados_operacao(dados)
            if success:
                st.success(MENSAGENS['dados_sincronizados'])
            else:
                st.warning("‚ö†Ô∏è Falha ao salvar no Supabase, salvando apenas localmente")
        
        # Sempre salvar localmente como fallback
        st.info("üíæ Salvando no arquivo local...")
        with open(DADOS['arquivo_saida'], 'w', encoding='utf-8') as f:
            json.dump(dados, f, ensure_ascii=False, indent=2)
        st.success(MENSAGENS['sucesso_salvar'])
        return True
            
    except Exception as e:
        st.error(f"‚ùå **ERRO AO SALVAR DADOS OPERA√á√ÉO:**")
        st.error(f"**Tipo:** {type(e).__name__}")
        st.error(f"**Mensagem:** {str(e)}")
        st.error("**Stack trace:**")
        import traceback
        st.code(traceback.format_exc())
        return False

def carregar_dados_operacao() -> list:
    """
    Carrega dados de opera√ß√£o do banco de dados ou arquivo local
    Prioriza dados locais quando s√£o mais recentes que os do banco
    """
    import time  # Importar time no in√≠cio da fun√ß√£o
    
    try:
        # Carregar dados locais primeiro para verificar se existem
        dados_locais = []
        if os.path.exists(DADOS['arquivo_saida']):
            try:
                with open(DADOS['arquivo_saida'], 'r', encoding='utf-8') as f:
                    dados_locais = json.load(f)
            except:
                dados_locais = []
        
        # Tentar carregar do Supabase
        dados_supabase = []
        if DB_AVAILABLE and db_manager.is_connected():
            status_placeholder = st.empty()
            status_placeholder.info("üîÑ Tentando carregar dados do Supabase...")
            
            try:
                dados_supabase = db_manager.load_dados_operacao()
                if dados_supabase:
                    status_placeholder.success(f"‚úÖ Carregados {len(dados_supabase)} registros do Supabase")
                else:
                    status_placeholder.warning("‚ö†Ô∏è Nenhum dado encontrado no Supabase")
            except Exception as e:
                status_placeholder.warning(f"‚ö†Ô∏è Erro ao carregar do Supabase: {e}")
                dados_supabase = []
            
            time.sleep(3)
            status_placeholder.empty()
        
        # Decidir qual fonte de dados usar
        if dados_locais and dados_supabase:
            # Comparar datas para decidir qual √© mais recente
            try:
                # Encontrar data mais recente dos dados locais
                datas_locais = [datetime.strptime(d['data'], '%Y-%m-%d') for d in dados_locais if 'data' in d]
                data_max_local = max(datas_locais) if datas_locais else None
                
                # Encontrar data mais recente dos dados do banco
                datas_banco = [datetime.strptime(d['data'], '%Y-%m-%d') for d in dados_supabase if 'data' in d]
                data_max_banco = max(datas_banco) if datas_banco else None
                
                if data_max_local and data_max_banco:
                    if data_max_local > data_max_banco:
                        st.info(f"üìÖ Dados locais s√£o mais recentes ({data_max_local.strftime('%d/%m/%Y')} vs {data_max_banco.strftime('%d/%m/%Y')})")
                        st.info("üíæ Usando dados locais")
                        return dados_locais
                    else:
                        st.info(f"üìÖ Dados do banco s√£o mais recentes ({data_max_banco.strftime('%d/%m/%Y')} vs {data_max_local.strftime('%d/%m/%Y')})")
                        st.info("üåê Usando dados do banco")
                        return dados_supabase
                else:
                    # Se n√£o conseguiu comparar datas, usar dados locais por seguran√ßa
                    st.info("üìÖ N√£o foi poss√≠vel comparar datas. Usando dados locais por seguran√ßa.")
                    return dados_locais
                    
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Erro ao comparar datas: {e}. Usando dados locais.")
                return dados_locais
        
        elif dados_locais:
            # S√≥ dados locais dispon√≠veis
            st.info(f"üíæ Carregados {len(dados_locais)} registros do arquivo local")
            return dados_locais
        
        elif dados_supabase:
            # S√≥ dados do banco dispon√≠veis
            st.info(f"üåê Carregados {len(dados_supabase)} registros do Supabase")
            return dados_supabase
        
        else:
            # Nenhum dado dispon√≠vel
            st.info("üìù Nenhum dado encontrado. Criando arquivo vazio...")
            # Criar arquivo vazio se n√£o existir
            if not os.path.exists(DADOS['arquivo_saida']):
                with open(DADOS['arquivo_saida'], 'w', encoding='utf-8') as f:
                    json.dump([], f, ensure_ascii=False, indent=2)
                st.success("‚úÖ Arquivo local criado com sucesso")
            return []
        
    except Exception as e:
        # Em caso de erro, tentar carregar dados locais
        st.error(f"‚ùå Erro ao carregar dados: {e}")
        try:
            if os.path.exists(DADOS['arquivo_saida']):
                with open(DADOS['arquivo_saida'], 'r', encoding='utf-8') as f:
                    dados = json.load(f)
                st.info(f"‚úÖ Carregados {len(dados)} registros do arquivo local ap√≥s erro")
                return dados
        except:
            pass
        
        # Criar arquivo vazio como √∫ltimo recurso
        try:
            with open(DADOS['arquivo_saida'], 'w', encoding='utf-8') as f:
                json.dump([], f, ensure_ascii=False, indent=2)
            st.info("‚úÖ Arquivo local criado ap√≥s erro")
            return []
        except Exception as e2:
            st.error(f"‚ùå Erro ao criar arquivo local: {e2}")
            return []

def salvar_dados_validacao(df: pd.DataFrame, arquivo_origem: str = None) -> bool:
    """
    Salva dados de valida√ß√£o no banco de dados
    """
    try:
        if DB_AVAILABLE and db_manager.is_connected():
            success = db_manager.save_dados_validacao(df, arquivo_origem)
            if success:
                st.success("‚úÖ Dados de valida√ß√£o salvos no banco!")
            return success
        else:
            st.warning("‚ö†Ô∏è Supabase n√£o conectado. Dados n√£o salvos no banco.")
            return False
            
    except Exception as e:
        st.error(f"‚ùå Erro ao salvar dados de valida√ß√£o: {e}")
        return False

def carregar_dados_validacao(limit: int = 1000) -> pd.DataFrame:
    """
    Carrega dados de valida√ß√£o do banco de dados
    """
    try:
        if DB_AVAILABLE and db_manager.is_connected():
            return db_manager.load_dados_validacao(limit)
        else:
            st.warning("‚ö†Ô∏è Supabase n√£o conectado. Carregando dados locais.")
            return pd.DataFrame()
            
    except Exception as e:
        st.error(f"‚ùå Erro ao carregar dados de valida√ß√£o: {e}")
        return pd.DataFrame()

def salvar_flutuantes_operador(flutuantes_data: dict, data_operacao: str) -> bool:
    """
    Salva dados de flutuantes por operador no banco
    """
    try:
        if DB_AVAILABLE and db_manager.is_connected():
            success = db_manager.save_flutuantes_operador(flutuantes_data, data_operacao)
            if success:
                st.success("‚úÖ Flutuantes por operador salvos no banco!")
            return success
        else:
            st.warning("‚ö†Ô∏è Supabase n√£o conectado. Dados n√£o salvos no banco.")
            return False
            
    except Exception as e:
        st.error(f"‚ùå Erro ao salvar flutuantes por operador: {e}")
        return False

def carregar_flutuantes_operador(data_operacao: str = None) -> dict:
    """
    Carrega dados de flutuantes por operador do banco
    """
    try:
        if DB_AVAILABLE and db_manager.is_connected():
            return db_manager.load_flutuantes_operador(data_operacao)
        else:
            st.warning("‚ö†Ô∏è Supabase n√£o conectado.")
            return {}
            
    except Exception as e:
        st.error(f"‚ùå Erro ao carregar flutuantes por operador: {e}")
        return {}

def sincronizar_dados_locais(dados_locais: list) -> bool:
    """
    Sincroniza dados locais com o Supabase
    """
    try:
        if DB_AVAILABLE and db_manager.is_connected():
            return db_manager.sync_local_to_supabase(dados_locais)
        else:
            st.warning("‚ö†Ô∏è Supabase n√£o conectado. Sincroniza√ß√£o n√£o poss√≠vel.")
            return False
            
    except Exception as e:
        st.error(f"‚ùå Erro na sincroniza√ß√£o: {e}")
        return False

def obter_estatisticas_banco() -> dict:
    """
    Obt√©m estat√≠sticas do banco de dados
    """
    try:
        if DB_AVAILABLE and db_manager.is_connected():
            return db_manager.get_estatisticas()
        else:
            return {}
            
    except Exception as e:
        st.error(f"‚ùå Erro ao obter estat√≠sticas: {e}")
        return {}

# ============================================================================
# FUN√á√ïES PARA PACOTES FLUTUANTES
# ============================================================================

def processar_csv_flutuantes(uploaded_file):
    """
    Processa upload de CSV de pacotes flutuantes e retorna dados formatados
    """
    try:
        if uploaded_file.name.endswith('.csv'):
            df = pd.read_csv(uploaded_file)
        else:
            st.error("Formato de arquivo n√£o suportado. Use .csv")
            return None
        
        # Verificar colunas obrigat√≥rias
        colunas_obrigatorias = [
            'Estacao', 'Semana', 'Data de Recebimento', 'Destino', 'Aging',
            'Tracking Number', 'Foi Expedido', 'Operador', 'Status SPX',
            'Status', 'Foi encontrado', 'Descricao do item', 'Operador Real'
        ]
        
        colunas_faltantes = [col for col in colunas_obrigatorias if col not in df.columns]
        if colunas_faltantes:
            st.error(f"Colunas obrigat√≥rias n√£o encontradas: {', '.join(colunas_faltantes)}")
            st.info("Colunas esperadas: " + ", ".join(colunas_obrigatorias))
            return None
        
        # Limpar e processar dados
        df = df.dropna(subset=['Operador Real'])  # Remover linhas sem operador real
        df = df[df['Operador Real'].str.strip() != '']  # Remover operadores vazios
        
        # Processar campo "Foi Expedido"
        if 'Foi Expedido' in df.columns:
            df['foi_expedido'] = df['Foi Expedido'].astype(str).str.strip()
            # Mapear valores espec√≠ficos do CSV
            mapeamento_expedido = {
                'Pacote Flutuante': False,
                'Pacote Revertido': True,
                'Sim': True,
                'sim': True,
                'S': True,
                's': True,
                'Yes': True,
                'yes': True,
                'Y': True,
                'y': True,
                '1': True,
                'TRUE': True,
                'True': True,
                'true': True,
                'N√£o': False,
                'Nao': False,
                'nao': False,
                'n√£o': False,
                'N': False,
                'n': False,
                'No': False,
                'no': False,
                '0': False,
                'FALSE': False,
                'False': False,
                'false': False
            }
            df['foi_expedido'] = df['foi_expedido'].map(mapeamento_expedido).fillna(False)
            # Remover coluna original
            df = df.drop('Foi Expedido', axis=1)
        
        # Processar campo "Foi encontrado" - este √© o campo principal para determinar se foi encontrado
        if 'Foi encontrado' in df.columns:
            st.write("üîç DEBUG: Processando campo 'Foi encontrado'")
            st.write(f"Valores √∫nicos originais: {df['Foi encontrado'].unique()}")
            
            df['foi_encontrado'] = df['Foi encontrado'].astype(str).str.strip()
            st.write(f"Valores √∫nicos ap√≥s convers√£o: {df['foi_encontrado'].unique()}")
            
            # Mapear valores espec√≠ficos do CSV
            mapeamento_encontrado = {
                'Sim': True,
                'sim': True,
                'S': True,
                's': True,
                'Yes': True,
                'yes': True,
                'Y': True,
                'y': True,
                '1': True,
                'TRUE': True,
                'True': True,
                'true': True,
                'N√£o': False,
                'Nao': False,
                'nao': False,
                'n√£o': False,
                'N': False,
                'n': False,
                'No': False,
                'no': False,
                '0': False,
                'FALSE': False,
                'False': False,
                'false': False
            }
            
            # Mostrar mapeamento aplicado
            for valor in df['foi_encontrado'].unique():
                if valor in mapeamento_encontrado:
                    st.write(f"Mapeamento: '{valor}' -> {mapeamento_encontrado[valor]}")
                else:
                    st.write(f"‚ö†Ô∏è Valor n√£o mapeado: '{valor}'")
            
            df['foi_encontrado'] = df['foi_encontrado'].map(mapeamento_encontrado).fillna(False)
            
            st.write(f"Resultado final - True: {(df['foi_encontrado'] == True).sum()}, False: {(df['foi_encontrado'] == False).sum()}")
            
            # Remover coluna original
            df = df.drop('Foi encontrado', axis=1)
        
        # Processar campo "Status" - manter como backup mas n√£o usar para c√°lculo
        if 'Status' in df.columns:
            df['status'] = df['Status'].astype(str).str.strip()
            # Mapear valores espec√≠ficos do CSV
            mapeamento_status = {
                'TRUE': True,
                'True': True,
                'true': True,
                'Sim': True,
                'sim': True,
                'S': True,
                's': True,
                'Yes': True,
                'yes': True,
                'Y': True,
                'y': True,
                '1': True,
                'FALSE': False,
                'False': False,
                'false': False,
                'N√£o': False,
                'Nao': False,
                'nao': False,
                'n√£o': False,
                'N': False,
                'n': False,
                'No': False,
                'no': False,
                '0': False
            }
            df['status'] = df['status'].map(mapeamento_status).fillna(False)
            # Remover coluna original
            df = df.drop('Status', axis=1)
        
        # Processar campo "Destino" - preencher vazios
        if 'Destino' in df.columns:
            df['Destino'] = df['Destino'].fillna('N√£o informado')
        
        # Processar campo "Aging" - garantir que seja num√©rico
        if 'Aging' in df.columns:
            df['Aging'] = pd.to_numeric(df['Aging'], errors='coerce').fillna(0).astype(int)
        
        if df.empty:
            st.error("Nenhum dado v√°lido encontrado ap√≥s limpeza")
            return None
        
        st.success(f"‚úÖ CSV processado com sucesso! {len(df)} registros v√°lidos encontrados")
        return df
        
    except Exception as e:
        st.error(f"Erro ao processar arquivo CSV: {e}")
        return None

def salvar_pacotes_flutuantes(df: pd.DataFrame, arquivo_origem: str = None, upsert: bool = True) -> bool:
    """
    Salva dados de pacotes flutuantes no banco de dados com op√ß√£o de upsert
    """
    try:
        if DB_AVAILABLE and db_manager.is_connected():
            success = db_manager.save_pacotes_flutuantes(df, arquivo_origem, upsert)
            if success:
                if upsert:
                    st.success("‚úÖ Pacotes flutuantes processados no banco (modo upsert)!")
                else:
                    st.success("‚úÖ Pacotes flutuantes salvos no banco!")
            return success
        else:
            st.warning("‚ö†Ô∏è Supabase n√£o conectado. Dados n√£o salvos no banco.")
            return False
            
    except Exception as e:
        st.error(f"‚ùå Erro ao salvar pacotes flutuantes: {e}")
        return False

def carregar_pacotes_flutuantes(limit: int = 1000, operador_real: str = None, data_inicio: str = None, data_fim: str = None) -> pd.DataFrame:
    """
    Carrega dados de pacotes flutuantes do banco de dados
    """
    try:
        if DB_AVAILABLE and db_manager.is_connected():
            return db_manager.load_pacotes_flutuantes(limit, operador_real, data_inicio, data_fim)
        else:
            st.warning("‚ö†Ô∏è Supabase n√£o conectado.")
            return pd.DataFrame()
            
    except Exception as e:
        st.error(f"‚ùå Erro ao carregar pacotes flutuantes: {e}")
        return pd.DataFrame()

def carregar_pacotes_flutuantes_multiplos_operadores(limit: int = 1000, operadores_reais: list = None, data_inicio: str = None, data_fim: str = None) -> pd.DataFrame:
    """
    Carrega dados de pacotes flutuantes do banco de dados com suporte a m√∫ltiplos operadores
    """
    try:
        if DB_AVAILABLE and db_manager.is_connected():
            return db_manager.load_pacotes_flutuantes_multiplos_operadores(limit, operadores_reais, data_inicio, data_fim)
        else:
            st.warning("‚ö†Ô∏è Supabase n√£o conectado.")
            return pd.DataFrame()
            
    except Exception as e:
        st.error(f"‚ùå Erro ao carregar pacotes flutuantes: {e}")
        return pd.DataFrame()

def obter_ranking_operadores_flutuantes() -> pd.DataFrame:
    """
    Obt√©m ranking de operadores com mais flutuantes
    """
    try:
        if DB_AVAILABLE and db_manager.is_connected():
            return db_manager.get_ranking_operadores_flutuantes()
        else:
            st.warning("‚ö†Ô∏è Supabase n√£o conectado.")
            return pd.DataFrame()
            
    except Exception as e:
        st.error(f"‚ùå Erro ao obter ranking de operadores: {e}")
        return pd.DataFrame()

def obter_resumo_flutuantes_estacao() -> pd.DataFrame:
    """
    Obt√©m resumo de flutuantes por esta√ß√£o
    """
    try:
        if DB_AVAILABLE and db_manager.is_connected():
            return db_manager.get_resumo_flutuantes_estacao()
        else:
            st.warning("‚ö†Ô∏è Supabase n√£o conectado.")
            return pd.DataFrame()
            
    except Exception as e:
        st.error(f"‚ùå Erro ao obter resumo por esta√ß√£o: {e}")
        return pd.DataFrame()

def obter_total_flutuantes_por_data(data_operacao: str) -> int:
    """
    Obt√©m total de flutuantes para uma data espec√≠fica
    """
    try:
        if DB_AVAILABLE and db_manager.is_connected():
            return db_manager.get_total_flutuantes_por_data(data_operacao)
        else:
            return 0
            
    except Exception as e:
        st.error(f"‚ùå Erro ao obter total de flutuantes: {e}")
        return 0

def exportar_flutuantes_excel(df: pd.DataFrame, nome_arquivo: str = "relatorio_flutuantes.xlsx"):
    """
    Exporta dados de flutuantes para arquivo Excel
    """
    try:
        # Criar c√≥pia para n√£o modificar o original
        df_export = df.copy()
        
        # Formatar colunas de data
        if 'data_recebimento' in df_export.columns:
            df_export['data_recebimento'] = pd.to_datetime(df_export['data_recebimento']).dt.strftime('%d/%m/%Y')
        
        if 'importado_em' in df_export.columns:
            df_export['importado_em'] = pd.to_datetime(df_export['importado_em']).dt.strftime('%d/%m/%Y %H:%M')
        
        # Renomear colunas para portugu√™s
        mapeamento_colunas = {
            'estacao': 'Esta√ß√£o',
            'semana': 'Semana',
            'data_recebimento': 'Data de Recebimento',
            'destino': 'Destino',
            'aging': 'Aging',
            'tracking_number': 'Tracking Number',
            'foi_expedido': 'Foi Expedido',
            'operador': 'Operador',
            'status_spx': 'Status SPX',
            'status': 'Status',
            'foi_encontrado': 'Foi Encontrado',
            'descricao_item': 'Descri√ß√£o do Item',
            'operador_real': 'Operador Real',
            'importado_em': 'Importado em',
            'arquivo_origem': 'Arquivo de Origem'
        }
        
        df_export = df_export.rename(columns=mapeamento_colunas)
        
        # Salvar Excel
        with pd.ExcelWriter(nome_arquivo, engine='xlsxwriter') as writer:
            df_export.to_excel(writer, sheet_name='Pacotes Flutuantes', index=False)
            
            # Formata√ß√£o
            workbook = writer.book
            worksheet = writer.sheets['Pacotes Flutuantes']
            
            # Formatos
            formato_header = workbook.add_format({
                'bold': True,
                'bg_color': '#EE4D2D',
                'font_color': 'white',
                'border': 1
            })
            
            formato_data = workbook.add_format({
                'num_format': 'dd/mm/yyyy',
                'border': 1
            })
            
            formato_texto = workbook.add_format({
                'border': 1
            })
            
            # Aplicar formatos
            for col_num, value in enumerate(df_export.columns.values):
                worksheet.write(0, col_num, value, formato_header)
            
            # Formatar colunas espec√≠ficas
            worksheet.set_column('A:A', 15, formato_texto)  # Esta√ß√£o
            worksheet.set_column('B:B', 10, formato_texto)  # Semana
            worksheet.set_column('C:C', 15, formato_data)   # Data de Recebimento
            worksheet.set_column('D:D', 15, formato_texto)  # Destino
            worksheet.set_column('E:E', 8, formato_texto)   # Aging
            worksheet.set_column('F:F', 20, formato_texto)  # Tracking Number
            worksheet.set_column('G:G', 12, formato_texto)  # Foi Expedido
            worksheet.set_column('H:H', 15, formato_texto)  # Operador
            worksheet.set_column('I:I', 15, formato_texto)  # Status SPX
            worksheet.set_column('J:J', 15, formato_texto)  # Status
            worksheet.set_column('K:K', 12, formato_texto)  # Foi Encontrado
            worksheet.set_column('L:L', 30, formato_texto)  # Descri√ß√£o do Item
            worksheet.set_column('M:M', 15, formato_texto)  # Operador Real
            worksheet.set_column('N:N', 20, formato_texto)  # Importado em
            worksheet.set_column('O:O', 20, formato_texto)  # Arquivo de Origem
        
        return nome_arquivo
        
    except Exception as e:
        st.error(f"Erro ao exportar dados: {e}")
        return None 

# ============================================================================
# FUN√á√ïES PARA DADOS DI√ÅRIOS VIA CSV
# ============================================================================

def processar_csv_dados_diarios(uploaded_file):
    """
    Processa upload de CSV de dados di√°rios de opera√ß√£o e retorna dados formatados
    Mapeamento das colunas:
    - Data -> data
    - quantidade de pacotes -> volume_veiculo (pacotes do dia)
    - backlog -> backlog (pacotes de dias anteriores)
    - flutuantes -> flutuantes (sem bipar)
    - encontrados -> flutuantes_revertidos (encontrados)
    - erros segundo sorting -> erros_sorting (gaiola errada)
    - Erros etiquetagem -> erros_etiquetagem
    """
    try:
        if uploaded_file.name.endswith('.csv'):
            df = pd.read_csv(uploaded_file)
        else:
            st.error("Formato de arquivo n√£o suportado. Use .csv")
            return None
        
        # Verificar se as colunas necess√°rias existem
        colunas_necessarias = [
            'Data', 'quantidade de pacotes', 'backlog', 'flutuantes', 
            'encontrados', 'erros segundo sorting', 'Erros etiquetagem'
        ]
        
        colunas_faltantes = [col for col in colunas_necessarias if col not in df.columns]
        if colunas_faltantes:
            st.error(f"Colunas obrigat√≥rias n√£o encontradas: {', '.join(colunas_faltantes)}")
            st.info("Colunas esperadas: " + ", ".join(colunas_necessarias))
            return None
        
        # Processar dados
        dados_processados = []
        for index, row in df.iterrows():
            try:
                # Converter data - tentar formato brasileiro primeiro (DD/MM/YYYY)
                try:
                    # Primeiro tenta formato brasileiro
                    data = pd.to_datetime(row['Data'], format='%d/%m/%Y').strftime('%Y-%m-%d')
                except:
                    try:
                        # Se falhar, tenta formato brasileiro com ano de 2 d√≠gitos
                        data = pd.to_datetime(row['Data'], format='%d/%m/%y').strftime('%Y-%m-%d')
                    except:
                        try:
                            # Se falhar, tenta formato ISO (YYYY-MM-DD)
                            data = pd.to_datetime(row['Data'], format='%Y-%m-%d').strftime('%Y-%m-%d')
                        except:
                            # √öltima tentativa: deixar o pandas tentar adivinhar
                            data = pd.to_datetime(row['Data']).strftime('%Y-%m-%d')
                
                # Converter valores num√©ricos
                quantidade_pacotes = int(row['quantidade de pacotes'])
                backlog = int(row['backlog'])
                flutuantes = int(row['flutuantes'])
                encontrados = int(row['encontrados'])
                erros_sorting = int(row['erros segundo sorting'])
                erros_etiquetagem = int(row['Erros etiquetagem'])
                
                # Calcular volume di√°rio total
                volume_diario = backlog + quantidade_pacotes
                
                dados_processados.append({
                    'data': data,
                    'backlog': backlog,
                    'volume_veiculo': quantidade_pacotes,
                    'volume_diario': volume_diario,
                    'flutuantes': flutuantes,
                    'flutuantes_revertidos': encontrados,
                    'erros_sorting': erros_sorting,
                    'erros_etiquetagem': erros_etiquetagem
                })
                
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Erro ao processar linha {index + 1}: {e}")
                continue
        
        if not dados_processados:
            st.error("‚ùå Nenhum dado v√°lido encontrado ap√≥s processamento")
            return None
        
        # Mostrar exemplo de convers√£o de data para o usu√°rio
        if dados_processados:
            primeira_data_original = df['Data'].iloc[0] if not df.empty else None
            primeira_data_processada = dados_processados[0]['data'] if dados_processados else None
            
            if primeira_data_original and primeira_data_processada:
                st.info(f"üìÖ **Exemplo de convers√£o de data:** `{primeira_data_original}` ‚Üí `{primeira_data_processada}`")
        
        st.success(f"‚úÖ CSV processado com sucesso! {len(dados_processados)} registros v√°lidos")
        return dados_processados
        
    except Exception as e:
        st.error(f"‚ùå Erro ao processar arquivo CSV: {e}")
        return None

def processar_multiplos_csvs_dados_diarios(uploaded_files):
    """Processa m√∫ltiplos arquivos CSV de dados di√°rios e retorna uma lista consolidada"""
    todos_dados = []
    
    for i, uploaded_file in enumerate(uploaded_files):
        try:
            dados = processar_csv_dados_diarios(uploaded_file)
            if dados is not None:
                # Adicionar identificador do arquivo
                for dado in dados:
                    dado['arquivo_origem'] = uploaded_file.name
                todos_dados.extend(dados)
                st.success(f"‚úÖ {uploaded_file.name} processado com sucesso!")
            else:
                st.error(f"‚ùå Erro ao processar {uploaded_file.name}")
        except Exception as e:
            st.error(f"‚ùå Erro ao processar {uploaded_file.name}: {e}")
    
    if todos_dados:
        st.success(f"üéâ **Consolida√ß√£o conclu√≠da!** Total de {len(todos_dados)} registros de {len(uploaded_files)} arquivos.")
        return todos_dados
    else:
        st.error("‚ùå Nenhum arquivo foi processado com sucesso.")
        return None 

def extrair_codigo_operador(nome_operador):
    """
    Extrai o c√≥digo do operador do formato [ops67892]NOME
    """
    if pd.isna(nome_operador) or not isinstance(nome_operador, str):
        return None
    
    # Procurar padr√£o [opsXXXX] no in√≠cio do nome
    match = re.match(r'\[([^\]]+)\]', nome_operador.strip())
    if match:
        return match.group(1).lower()  # Retorna o c√≥digo em lowercase
    return None

def normalizar_nome_operador(nome_operador):
    """
    Normaliza o nome do operador removendo acentos e caracteres especiais
    """
    if pd.isna(nome_operador) or not isinstance(nome_operador, str):
        return nome_operador
    
    # Extrair apenas a parte do nome (ap√≥s o c√≥digo)
    codigo_match = re.match(r'\[([^\]]+)\](.+)', nome_operador.strip())
    if codigo_match:
        codigo = codigo_match.group(1)
        nome = codigo_match.group(2).strip()
        
        # Normalizar o nome removendo acentos
        nome_normalizado = unicodedata.normalize('NFKD', nome)
        nome_normalizado = ''.join([c for c in nome_normalizado if not unicodedata.combining(c)])
        
        # Capitalizar corretamente
        nome_normalizado = ' '.join([palavra.capitalize() for palavra in nome_normalizado.split()])
        
        return f"[{codigo}]{nome_normalizado}"
    
    return nome_operador

def agrupar_operadores_duplicados(df):
    """
    Agrupa operadores duplicados com base no c√≥digo identificador
    """
    if df.empty or 'operador_real' not in df.columns:
        return df
    
    # Criar c√≥pia para n√£o modificar o original
    df_agrupado = df.copy()
    
    # Extrair c√≥digos e normalizar nomes
    df_agrupado['codigo_operador'] = df_agrupado['operador_real'].apply(extrair_codigo_operador)
    df_agrupado['operador_normalizado'] = df_agrupado['operador_real'].apply(normalizar_nome_operador)
    
    # Criar mapeamento de operadores duplicados
    operadores_com_codigo = df_agrupado[df_agrupado['codigo_operador'].notna()]
    
    if not operadores_com_codigo.empty:
        # Encontrar o nome "principal" para cada c√≥digo (o mais comum ou o normalizado)
        nome_principal_por_codigo = operadores_com_codigo.groupby('codigo_operador')['operador_normalizado'].agg(
            lambda x: x.value_counts().index[0] if len(x.value_counts()) > 0 else x.iloc[0]
        ).to_dict()
        
        # Mapear todos os nomes variantes para o nome principal
        df_agrupado['operador_final'] = df_agrupado.apply(lambda row: 
            nome_principal_por_codigo.get(row['codigo_operador'], row['operador_real'])
            if row['codigo_operador'] is not None 
            else row['operador_real'], axis=1
        )
        
        # Substituir a coluna original
        df_agrupado['operador_real'] = df_agrupado['operador_final']
        
        # Remover colunas auxiliares
        df_agrupado = df_agrupado.drop(['codigo_operador', 'operador_normalizado', 'operador_final'], axis=1)
    
    return df_agrupado

def obter_estatisticas_duplicacao_operadores(df):
    """
    Retorna estat√≠sticas sobre operadores duplicados encontrados
    """
    if df.empty or 'operador_real' not in df.columns:
        return {}
    
    # Extrair c√≥digos
    df_temp = df.copy()
    df_temp['codigo_operador'] = df_temp['operador_real'].apply(extrair_codigo_operador)
    
    # Contar operadores √∫nicos vs c√≥digos √∫nicos
    operadores_com_codigo = df_temp[df_temp['codigo_operador'].notna()]
    
    if operadores_com_codigo.empty:
        return {
            'total_operadores_unicos': df['operador_real'].nunique(),
            'operadores_com_codigo': 0,
            'codigos_unicos': 0,
            'duplicados_encontrados': 0
        }
    
    # Encontrar duplicados
    duplicados_por_codigo = operadores_com_codigo.groupby('codigo_operador')['operador_real'].nunique()
    duplicados_encontrados = duplicados_por_codigo[duplicados_por_codigo > 1]
    
    return {
        'total_operadores_unicos': df['operador_real'].nunique(),
        'operadores_com_codigo': operadores_com_codigo['operador_real'].nunique(),
        'codigos_unicos': operadores_com_codigo['codigo_operador'].nunique(),
        'duplicados_encontrados': len(duplicados_encontrados),
        'detalhes_duplicados': duplicados_encontrados.to_dict() if len(duplicados_encontrados) > 0 else {}
    } 

def diagnosticar_operador(df, nome_busca):
    """
    Diagnostica problemas com um operador espec√≠fico
    """
    if df.empty or 'operador_real' not in df.columns:
        return {
            'erro': 'DataFrame vazio ou sem coluna operador_real',
            'operadores_encontrados': []
        }
    
    # Buscar operadores que contenham o nome
    operadores_similar = df[df['operador_real'].str.contains(nome_busca, case=False, na=False)]['operador_real'].unique()
    
    # Buscar operadores com c√≥digos similares
    df_temp = df.copy()
    df_temp['codigo_operador'] = df_temp['operador_real'].apply(extrair_codigo_operador)
    df_temp['nome_sem_codigo'] = df_temp['operador_real'].apply(lambda x: 
        re.sub(r'\[[^\]]+\]', '', str(x)).strip() if pd.notna(x) else ''
    )
    
    # Buscar por nome sem c√≥digo
    nomes_similar = df_temp[df_temp['nome_sem_codigo'].str.contains(nome_busca, case=False, na=False)]['operador_real'].unique()
    
    # Todos os operadores √∫nicos
    todos_operadores = sorted(df['operador_real'].dropna().unique())
    
    # Estat√≠sticas do operador se encontrado
    dados_operador = {}
    for op in set(list(operadores_similar) + list(nomes_similar)):
        dados_op = df[df['operador_real'] == op]
        if not dados_op.empty:
            dados_operador[op] = {
                'total_registros': len(dados_op),
                'total_flutuantes': len(dados_op),
                'encontrados': dados_op['foi_encontrado'].sum() if 'foi_encontrado' in dados_op.columns else 0,
                'datas': {
                    'primeira': dados_op['data_recebimento'].min() if 'data_recebimento' in dados_op.columns else None,
                    'ultima': dados_op['data_recebimento'].max() if 'data_recebimento' in dados_op.columns else None
                }
            }
    
    return {
        'nome_buscado': nome_busca,
        'operadores_exatos': list(operadores_similar),
        'operadores_similares': list(nomes_similar),
        'total_operadores_base': len(todos_operadores),
        'dados_operadores': dados_operador,
        'primeiros_10_operadores': todos_operadores[:10],
        'operadores_com_monica': [op for op in todos_operadores if 'monica' in op.lower()],
        'todos_operadores_unicos': todos_operadores
    }

def verificar_normalizacao_operador(nome_original):
    """
    Verifica como um nome de operador seria normalizado
    """
    return {
        'nome_original': nome_original,
        'codigo_extraido': extrair_codigo_operador(nome_original),
        'nome_normalizado': normalizar_nome_operador(nome_original)
    }

def buscar_operadores_por_padrao(df, padrao):
    """
    Busca operadores usando diferentes padr√µes
    """
    if df.empty or 'operador_real' not in df.columns:
        return []
    
    resultados = []
    
    # Busca exata
    exatos = df[df['operador_real'] == padrao]['operador_real'].unique()
    resultados.extend([('exato', op) for op in exatos])
    
    # Busca case-insensitive
    case_insensitive = df[df['operador_real'].str.lower() == padrao.lower()]['operador_real'].unique()
    resultados.extend([('case_insensitive', op) for op in case_insensitive if op not in exatos])
    
    # Busca com contains
    contains = df[df['operador_real'].str.contains(padrao, case=False, na=False)]['operador_real'].unique()
    resultados.extend([('contains', op) for op in contains if op not in [x[1] for x in resultados]])
    
    # Busca por partes (sem c√≥digo)
    sem_codigo = df['operador_real'].apply(lambda x: re.sub(r'\[[^\]]+\]', '', str(x)).strip() if pd.notna(x) else '')
    por_partes = df[sem_codigo.str.contains(padrao, case=False, na=False)]['operador_real'].unique()
    resultados.extend([('sem_codigo', op) for op in por_partes if op not in [x[1] for x in resultados]])
    
    return resultados 

def criar_mapeamento_operadores_originais(df):
    """
    Cria mapeamento de operadores normalizados para seus nomes originais no banco
    """
    if df.empty or 'operador_real' not in df.columns:
        return {}
    
    mapeamento = {}
    operadores_originais = df['operador_real'].dropna().unique()
    
    for operador_original in operadores_originais:
        # Normalizar o nome
        operador_normalizado = normalizar_nome_operador(operador_original)
        
        # Mapear normalizado -> original
        if operador_normalizado != operador_original:
            mapeamento[operador_normalizado] = operador_original
        
        # Tamb√©m mapear o c√≥digo para facilitar busca
        codigo = extrair_codigo_operador(operador_original)
        if codigo:
            # Mapear varia√ß√µes poss√≠veis do mesmo c√≥digo
            base_nome = re.sub(r'\[[^\]]+\]', '', operador_original).strip()
            for variacao in [base_nome.lower(), base_nome.upper(), base_nome.title()]:
                nome_variacao = f"[{codigo}]{variacao}"
                if nome_variacao != operador_original:
                    mapeamento[nome_variacao] = operador_original
    
    return mapeamento

def mapear_operadores_para_banco(operadores_selecionados, df_original):
    """
    Mapeia operadores normalizados/selecionados para nomes reais no banco
    """
    if not operadores_selecionados:
        return []
    
    # Criar mapeamento
    mapeamento = criar_mapeamento_operadores_originais(df_original)
    
    operadores_mapeados = []
    for operador in operadores_selecionados:
        # Verificar se precisa mapear
        if operador in mapeamento:
            operador_real = mapeamento[operador]
            operadores_mapeados.append(operador_real)
            st.info(f"üîÑ Mapeado: '{operador}' ‚Üí '{operador_real}'")
        else:
            # Usar o operador como est√°
            operadores_mapeados.append(operador)
    
    return operadores_mapeados

def carregar_pacotes_flutuantes_com_mapeamento(limit: int = 1000, operadores_reais: list = None, data_inicio: str = None, data_fim: str = None) -> pd.DataFrame:
    """
    Carrega pacotes flutuantes com mapeamento autom√°tico de operadores
    """
    try:
        if not DB_AVAILABLE or not db_manager.is_connected():
            st.warning("‚ö†Ô∏è Supabase n√£o conectado.")
            return pd.DataFrame()
        
        # Se h√° operadores selecionados, fazer mapeamento primeiro
        if operadores_reais and len(operadores_reais) > 0:
            # Carregar dados completos para criar mapeamento
            st.info("üîç Carregando dados para mapeamento de operadores...")
            df_completo = db_manager.load_pacotes_flutuantes(10000, None, None, None)
            
            if not df_completo.empty:
                # Mapear operadores selecionados para nomes reais no banco
                operadores_mapeados = mapear_operadores_para_banco(operadores_reais, df_completo)
                st.success(f"‚úÖ Operadores mapeados: {operadores_mapeados}")
                
                # Usar fun√ß√£o de m√∫ltiplos operadores com nomes mapeados
                return db_manager.load_pacotes_flutuantes_multiplos_operadores(limit, operadores_mapeados, data_inicio, data_fim)
            else:
                st.error("‚ùå N√£o foi poss√≠vel carregar dados para mapeamento")
                return pd.DataFrame()
        else:
            # Sem filtro de operadores, usar fun√ß√£o normal
            return db_manager.load_pacotes_flutuantes(limit, None, data_inicio, data_fim)
            
    except Exception as e:
        st.error(f"‚ùå Erro ao carregar pacotes flutuantes com mapeamento: {e}")
        return pd.DataFrame()

# ============================================================================
# FUN√á√ïES AUXILIARES PARA CONVERS√ÉO DE TIPOS
# ============================================================================

def converter_tipos_python(dados):
    """
    Converte tipos numpy/pandas para tipos Python nativos para evitar problemas de serializa√ß√£o JSON
    """
    if isinstance(dados, dict):
        return {chave: converter_tipos_python(valor) for chave, valor in dados.items()}
    elif isinstance(dados, list):
        return [converter_tipos_python(item) for item in dados]
    elif hasattr(dados, 'item'):  # numpy types
        return dados.item()
    elif hasattr(dados, 'dtype'):  # pandas types
        if pd.isna(dados):
            return None
        return dados.item() if hasattr(dados, 'item') else dados
    elif isinstance(dados, (np.integer, np.floating, np.bool_)):
        return dados.item()
    elif isinstance(dados, np.ndarray):
        return dados.tolist()
    else:
        return dados

# ============================================================================
# FUN√á√ïES PARA EXPEDI√á√ÉO CONSOLIDADO
# ============================================================================

def processar_csv_expedicao_consolidado(df_expedicao: pd.DataFrame, arquivo_origem: str) -> tuple:
    """
    Processa CSV de expedi√ß√£o para gerar dados consolidados
    Retorna: (dados_ondas, dados_operadores)
    """
    try:
        st.info("üîÑ Processando dados para expedi√ß√£o consolidado...")
        
        # Converter colunas de data
        df_expedicao['Validation Start Time'] = pd.to_datetime(df_expedicao['Validation Start Time'])
        df_expedicao['Validation End Time'] = pd.to_datetime(df_expedicao['Validation End Time'])
        df_expedicao['Delivering Time'] = pd.to_datetime(df_expedicao['Delivering Time'])
        
        # Adicionar coluna de data
        df_expedicao['Data'] = df_expedicao['Validation Start Time'].dt.date
        
        # Extrair letra da onda (primeira letra do Corridor Cage)
        df_expedicao['Onda'] = df_expedicao['Corridor Cage'].str.extract(r'^([A-Z])')[0]
        
        # Calcular tempo de confer√™ncia em minutos
        df_expedicao['Tempo_Conferencia_Min'] = (df_expedicao['Validation End Time'] - df_expedicao['Validation Start Time']).dt.total_seconds() / 60
        
        # Calcular tempo no piso (entre confer√™ncia e retirada)
        df_expedicao['Tempo_No_Piso_Min'] = (df_expedicao['Delivering Time'] - df_expedicao['Validation End Time']).dt.total_seconds() / 60
        
        # Agrupar por data e onda para consolidar
        dados_ondas = []
        dados_operadores = []
        
        # Processar cada data
        for data_operacao in df_expedicao['Data'].unique():
            df_data = df_expedicao[df_expedicao['Data'] == data_operacao]
            
            # Agrupar por onda nesta data
            for onda_letra in df_data['Onda'].unique():
                df_onda = df_data[df_data['Onda'] == onda_letra]
                
                # Calcular m√©tricas da onda
                hora_inicio = df_onda['Validation Start Time'].min()
                hora_fim = df_onda['Validation End Time'].max()
                tempo_total = (hora_fim - hora_inicio).total_seconds() / 60
                total_at_to = len(df_onda)
                total_pacotes = df_onda['Total Scanned Orders'].sum()
                operadores_ativos = df_onda['Validation Operator'].nunique()
                operadores_utilizados = df_onda['Validation Operator'].unique().tolist()
                
                # Calcular tempos m√©dios
                tempo_medio_por_at_to = df_onda['Tempo_Conferencia_Min'].mean()
                tempo_medio_por_pacote = tempo_total / total_pacotes if total_pacotes > 0 else 0
                
                # Determinar n√∫mero da onda (sequencial por data)
                ondas_data = sorted(df_data['Onda'].unique())
                numero_onda = ondas_data.index(onda_letra) + 1
                
                # Criar registro da onda
                dados_onda = {
                    'data_operacao': data_operacao.strftime('%Y-%m-%d'),
                    'numero_onda': numero_onda,
                    'letra_onda': onda_letra,
                    'hora_inicio': hora_inicio.isoformat(),
                    'hora_fim': hora_fim.isoformat(),
                    'tempo_total_minutos': round(tempo_total, 2),
                    'total_at_to': total_at_to,
                    'total_pacotes': total_pacotes,
                    'operadores_ativos': operadores_ativos,
                    'operadores_utilizados': operadores_utilizados,
                    'tempo_medio_por_at_to': round(tempo_medio_por_at_to, 2),
                    'tempo_medio_por_pacote': round(tempo_medio_por_pacote, 2),
                    'status_onda': 'Finalizada',
                    'observacoes': f'Onda {onda_letra} processada em {tempo_total:.1f} minutos',
                    'arquivo_origem': arquivo_origem
                }
                
                dados_ondas.append(dados_onda)
                
                # Processar dados dos operadores nesta onda
                for operador in df_onda['Validation Operator'].unique():
                    df_operador = df_onda[df_onda['Validation Operator'] == operador]
                    
                    # Calcular m√©tricas do operador
                    total_at_to_operador = len(df_operador)
                    total_pacotes_operador = df_operador['Total Scanned Orders'].sum()
                    tempo_total_operador = df_operador['Tempo_Conferencia_Min'].sum()
                    tempo_medio_por_at_to_operador = df_operador['Tempo_Conferencia_Min'].mean()
                    tempo_medio_por_pacote_operador = tempo_total_operador / total_pacotes_operador if total_pacotes_operador > 0 else 0
                    
                    # Calcular efici√™ncia do operador (score 0-100)
                    # Baseado no tempo m√©dio e quantidade de AT/TO
                    tempo_base = 50  # 50 minutos √© o target
                    eficiencia_tempo = max(0, 100 - ((tempo_medio_por_at_to_operador - tempo_base) / tempo_base * 100))
                    eficiencia_volume = min(100, (total_at_to_operador / 10) * 100)  # 10 AT/TO = 100%
                    eficiencia_operador = (eficiencia_tempo + eficiencia_volume) / 2
                    
                    # Calcular posi√ß√£o no ranking do dia (baseado na efici√™ncia)
                    operadores_dia = df_data['Validation Operator'].unique()
                    eficiencias_dia = []
                    for op in operadores_dia:
                        df_op = df_data[df_data['Validation Operator'] == op]
                        tempo_op = df_op['Tempo_Conferencia_Min'].mean()
                        at_to_op = len(df_op)
                        efic_tempo = max(0, 100 - ((tempo_op - tempo_base) / tempo_base * 100))
                        efic_volume = min(100, (at_to_op / 10) * 100)
                        eficiencias_dia.append((op, (efic_tempo + efic_volume) / 2))
                    
                    # Ordenar por efici√™ncia e encontrar posi√ß√£o
                    eficiencias_dia.sort(key=lambda x: x[1], reverse=True)
                    posicao_ranking = next((i + 1 for i, (op, _) in enumerate(eficiencias_dia) if op == operador), 999)
                    
                    # Criar registro do operador
                    dados_operador = {
                        'data_operacao': data_operacao.strftime('%Y-%m-%d'),
                        'operador': operador,
                        'numero_onda': numero_onda,
                        'total_at_to_expedidos': total_at_to_operador,
                        'total_pacotes_processados': total_pacotes_operador,
                        'tempo_total_trabalho_minutos': round(tempo_total_operador, 2),
                        'tempo_medio_por_at_to': round(tempo_medio_por_at_to_operador, 2),
                        'tempo_medio_por_pacote': round(tempo_medio_por_pacote_operador, 2),
                        'eficiencia_operador': round(eficiencia_operador, 2),
                        'posicao_ranking': posicao_ranking,
                        'arquivo_origem': arquivo_origem
                    }
                    
                    dados_operadores.append(dados_operador)
        
        # Converter tipos para Python nativos antes de retornar
        dados_ondas_convertidos = converter_tipos_python(dados_ondas)
        dados_operadores_convertidos = converter_tipos_python(dados_operadores)
        
        st.success(f"‚úÖ Processamento conclu√≠do!")
        st.info(f"  üìä Ondas processadas: {len(dados_ondas_convertidos)}")
        st.info(f"  üë• Registros de operadores: {len(dados_operadores_convertidos)}")
        
        return dados_ondas_convertidos, dados_operadores_convertidos
        
    except Exception as e:
        st.error(f"‚ùå Erro ao processar dados consolidados: {e}")
        return [], []

def salvar_expedicao_consolidado(dados_ondas: list, dados_operadores: list, arquivo_origem: str) -> bool:
    """
    Salva dados consolidados de expedi√ß√£o no banco de dados
    """
    try:
        if not DB_AVAILABLE or not db_manager.is_connected():
            st.warning("‚ö†Ô∏è Supabase n√£o conectado. Dados n√£o podem ser salvos.")
            return False
        
        # Salvar usando a fun√ß√£o do database manager
        success = db_manager.salvar_expedicao_consolidado(dados_ondas, dados_operadores, arquivo_origem)
        
        if success:
            st.success("‚úÖ Dados consolidados salvos com sucesso no banco!")
            return True
        else:
            st.error("‚ùå Falha ao salvar dados consolidados!")
            return False
            
    except Exception as e:
        st.error(f"‚ùå Erro ao salvar dados consolidados: {e}")
        return False

def carregar_expedicao_consolidado(data_inicio: str = None, data_fim: str = None) -> pd.DataFrame:
    """
    Carrega dados consolidados de expedi√ß√£o do banco
    """
    try:
        if not DB_AVAILABLE or not db_manager.is_connected():
            st.warning("‚ö†Ô∏è Supabase n√£o conectado.")
            return pd.DataFrame()
        
        return db_manager.carregar_expedicao_consolidado(data_inicio, data_fim)
        
    except Exception as e:
        st.error(f"‚ùå Erro ao carregar dados consolidados: {e}")
        return pd.DataFrame()

def obter_recomendacao_operadores_top_6(data_referencia: str = None) -> pd.DataFrame:
    """
    Obt√©m recomenda√ß√£o dos 6 melhores operadores para iniciar ondas
    Baseado no hist√≥rico de performance
    """
    try:
        if not DB_AVAILABLE or not db_manager.is_connected():
            st.warning("‚ö†Ô∏è Supabase n√£o conectado.")
            return pd.DataFrame()
        
        # Carregar ranking de operadores
        df_ranking = db_manager.obter_ranking_expedicao_operadores()
        
        if df_ranking.empty:
            st.info("üìù Nenhum dado de ranking dispon√≠vel.")
            return pd.DataFrame()
        
        # Filtrar operadores com pelo menos 1 dia de trabalho (mais flex√≠vel)
        dias_minimos = 1
        df_filtrado = df_ranking[df_ranking['dias_trabalhados'] >= dias_minimos].copy()
        
        if df_filtrado.empty:
            st.info(f"üìù Nenhum operador com pelo menos {dias_minimos} dia(s) de trabalho encontrado.")
            return pd.DataFrame()
        
        # Ajustar pesos baseado na quantidade de dias dispon√≠veis
        if df_filtrado['dias_trabalhados'].max() < 3:
            st.info("üìä **Aten√ß√£o:** Dados limitados (menos de 3 dias). Recomenda√ß√µes baseadas em performance recente.")
            # Ajustar pesos para dados limitados
            peso_eficiencia = 0.6  # Aumentar peso da efici√™ncia
            peso_frequencia = 0.2  # Reduzir peso da frequ√™ncia
            peso_volume = 0.15     # Manter volume
            peso_velocidade = 0.05 # Reduzir velocidade
        else:
            # Pesos normais para dados suficientes
            peso_eficiencia = 0.4
            peso_frequencia = 0.3
            peso_volume = 0.2
            peso_velocidade = 0.1
        
        # Calcular score composto para ranking usando pesos din√¢micos
        df_filtrado['score_composto'] = (
            df_filtrado['eficiencia_media'] * peso_eficiencia +
            (df_filtrado['percentual_top_6'] * peso_frequencia) +
            (df_filtrado['total_at_to_carreira'] / 100 * peso_volume) +
            (100 - df_filtrado['tempo_medio_por_at_to_carreira']) * peso_velocidade
        )
        
        # Ordenar por score e pegar top 6
        df_top_6 = df_filtrado.nlargest(6, 'score_composto').copy()
        
        # Adicionar justificativa para cada operador
        justificativas = []
        for _, row in df_top_6.iterrows():
            if row['eficiencia_media'] >= 80:
                justificativa = "üü¢ Alta efici√™ncia"
            elif row['eficiencia_media'] >= 60:
                justificativa = "üü° Boa efici√™ncia"
            else:
                justificativa = "üü† Efici√™ncia regular"
            
            # Adicionar informa√ß√£o sobre dias trabalhados
            if row['dias_trabalhados'] == 1:
                justificativa += " | üìÖ 1 dia de trabalho"
            else:
                justificativa += f" | üìÖ {row['dias_trabalhados']} dias de trabalho"
            
            if row['percentual_top_6'] >= 70:
                justificativa += " | üèÜ Frequente no top 6"
            elif row['percentual_top_6'] >= 40:
                justificativa += " | ü•â Ocasional no top 6"
            elif row['dias_trabalhados'] == 1:
                justificativa += " | üÜï Primeiro dia avaliado"
            
            if row['total_at_to_carreira'] >= 100:
                justificativa += " | üì¶ Alto volume de trabalho"
            elif row['total_at_to_carreira'] >= 50:
                justificativa += " | üì¶ Bom volume de trabalho"
            
            justificativas.append(justificativa)
        
        df_top_6['justificativa'] = justificativas
        
        return df_top_6
        
    except Exception as e:
        st.error(f"‚ùå Erro ao obter recomenda√ß√£o de operadores: {e}")
        return pd.DataFrame() 

# ============================================================================
# FUN√á√ïES AUXILIARES PARA FORMATA√á√ÉO E AN√ÅLISE
# ============================================================================

def formatar_tempo_minutos(minutos: float) -> str:
    """
    Formata tempo em minutos para exibi√ß√£o leg√≠vel
    - Se < 60 min: mostra em minutos
    - Se >= 60 min: mostra em horas e minutos
    """
    if minutos < 60:
        return f"{minutos:.1f} min"
    else:
        horas = int(minutos // 60)
        mins = int(minutos % 60)
        if mins == 0:
            return f"{horas}h"
        else:
            return f"{horas}h {mins}min"

def calcular_tempo_para_target(tempo_atual: float, target: float = 50) -> str:
    """
    Calcula quanto tempo falta para atingir o target
    Retorna string formatada com diferen√ßa
    """
    if tempo_atual <= target:
        return f"(‚úÖ {target - tempo_atual:.1f}min abaixo do target)"
    else:
        return f"(‚ùå {tempo_atual - target:.1f}min acima do target)"

def analisar_evolucao_tempo(dados_consolidados: pd.DataFrame) -> dict:
    """
    Analisa a evolu√ß√£o do tempo das ondas ao longo dos dias
    Retorna dicion√°rio com an√°lise de tend√™ncia
    """
    if dados_consolidados.empty:
        return {}
    
    try:
        # Ordenar por data
        df_ordenado = dados_consolidados.sort_values('data_operacao')
        
        # Calcular tempo m√©dio por dia
        df_diario = df_ordenado.groupby('data_operacao')['tempo_total_minutos'].mean().reset_index()
        
        if len(df_diario) < 2:
            return {'tendencia': 'insuficiente', 'mensagem': 'Dados insuficientes para an√°lise de tend√™ncia'}
        
        # Calcular tend√™ncia (primeira metade vs segunda metade)
        meio = len(df_diario) // 2
        primeira_metade = df_diario.iloc[:meio]['tempo_total_minutos'].mean()
        segunda_metade = df_diario.iloc[meio:]['tempo_total_minutos'].mean()
        
        # Calcular varia√ß√£o percentual
        if primeira_metade > 0:
            variacao_percentual = ((segunda_metade - primeira_metade) / primeira_metade) * 100
        else:
            variacao_percentual = 0
        
        # Determinar tend√™ncia
        if variacao_percentual < -5:
            tendencia = 'melhorando'
            emoji = 'üìà'
            cor = 'verde'
        elif variacao_percentual > 5:
            tendencia = 'piorando'
            emoji = 'üìâ'
            cor = 'vermelho'
        else:
            tendencia = 'estavel'
            emoji = '‚û°Ô∏è'
            cor = 'amarelo'
        
        # Calcular dias para target (proje√ß√£o)
        dias_para_target = None
        if tendencia == 'melhorando' and segunda_metade > 50:
            # Se est√° melhorando mas ainda acima do target, calcular proje√ß√£o
            melhoria_diaria = (primeira_metade - segunda_metade) / meio
            if melhoria_diaria > 0:
                dias_para_target = max(1, int((segunda_metade - 50) / melhoria_diaria))
        
        return {
            'tendencia': tendencia,
            'emoji': emoji,
            'cor': cor,
            'primeira_metade': primeira_metade,
            'segunda_metade': segunda_metade,
            'variacao_percentual': variacao_percentual,
            'dias_para_target': dias_para_target,
            'mensagem': f"{emoji} Tend√™ncia: {tendencia.upper()}"
        }
        
    except Exception as e:
        return {'tendencia': 'erro', 'mensagem': f'Erro na an√°lise: {str(e)}'} 